{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipos de funciones de activación\n",
    "\n",
    "1. **Función de activación de paso unitario:** aunque no muy utilizada, imita el comportamiento de una neurona biológica, que devuelve una señal o no dependiendo si el estímulo de entrada alcanza o no cierto umbral. La forma funcional que implementa esta idea es:\n",
    "\n",
    "$$f(x) = \n",
    "\\left\\{\n",
    "  \\begin{array}{lr}\n",
    "    0 & si\\ x < 0\\\\\n",
    "    1 & si\\ x \\ge 0\n",
    "  \\end{array}\n",
    "\\right.$$\n",
    "\n",
    "2. **Función de activación sigmoide:** una de las más utilizadas, permite que la señal de salida no sea binaria, sino que dependa de la magnitud de la entrada. \n",
    "\n",
    "$$f(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "3. **Función de activación de sigmoide - tangente hiperbólica:**\n",
    "\n",
    "$$f(x) = \\frac{2}{1-e^{-2x}} -1$$\n",
    "\n",
    "4. **Función de activación soft-max**: es una función que se aplica sobre la últiima capa de una red neuronal, que permite generar como respuesta una distribución de probabilidad, o sea ofrece (para un problema de clasificación) la probabilidad de pertenecer a cada clase. (ver el detalle de la capa en clase 3-1.38). Tiene una función de costo asociada una función de activación NLL que coincide con la entropía cruzada binaria).\n",
    "\n",
    "5. **Función RELU**: Se utiliza cuando hay muchas capas y muchas neuronas. \n",
    "\n",
    "6. **Función LeakyRelu**\n",
    "\n",
    "Algunas cuestiones a considerar de las funciones de activación:\n",
    "\n",
    "1. Al parecer el resultado de un modelo de redes neuronales **no depende en gran medida de la función de activación utilizada** (ver @lantz-2015, p.225). \n",
    "\n",
    "2. Dependiendo la función de activación, existe un fenómeno denominado **squashing**. Este expresa la situación en que la forma funcional genera una señal de salida de un rango limitado de valores, lo que puede llegar a *comprimir una señal de entrada*, especialmente para rangos de valores específicos (generalmente valores alejados del centro). Con esto se pierde información de los valores no centrales. Para evitar esto, se recomienda la **estandarización o normalización de las variables de entrada** (Lantz(2015), p.225). La función sigmoide es un ejemplo de función que genera este efecto."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
